{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9317b29",
   "metadata": {},
   "source": [
    "### Kaggle competition DSCC 483\n",
    "### Mini Project - Fall 2022\n",
    "### Tapan Pradyot\n",
    "### Hailey Thanki\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15875736",
   "metadata": {},
   "source": [
    "### This Notebook Contains the Primary Model used for generating the Final Kaggle Submission."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a4d326",
   "metadata": {},
   "source": [
    "###### References:\n",
    "###### https://www.nltk.org/api/nltk.tag.html\n",
    "#### Please see relavant code sections for references\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b9c1d3",
   "metadata": {},
   "source": [
    "##### Importing libraries pandas and nltk and reading the csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a48e01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sentence_transformers\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib as plt\n",
    "import nltk\n",
    "from nltk import sent_tokenize\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "import re, string\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import wordnet\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score, confusion_matrix\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score# bag of words\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer#for word embedding\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import wordnet\n",
    "from tqdm.notebook import tqdm\n",
    "from tqdm.auto import trange\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score,classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer \n",
    "from tqdm import tqdm\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.utils import to_categorical \n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8192eda5",
   "metadata": {},
   "source": [
    "#### Enabling the GPU on the laptop\n",
    "<b>reference:<b> https://www.tensorflow.org/guide/gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae36d200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Memory growth must be set before GPUs have been initialized\n",
    "    print(e)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53501c4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c687678",
   "metadata": {},
   "source": [
    "#### Reading the Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "550a3e06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tapan\\AppData\\Local\\Temp/ipykernel_25516/3588282187.py:1: DtypeWarning: Columns (2,3) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df=pd.read_csv(\"training_data.csv\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>reply_to_screen_name</th>\n",
       "      <th>is_quote</th>\n",
       "      <th>is_retweet</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Remember the #WuhanCoronaVirus? The pandemic w...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>WuhanCoronaVirus KillerCuomo</td>\n",
       "      <td>us</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>My sources @WhiteHouse say 2 tactics will be u...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>Trump</td>\n",
       "      <td>us</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I'll venture a wild guess: If you were running...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>COVID19</td>\n",
       "      <td>us</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#Pakistan (#GreenStimulus = #Nature protection...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>Pakistan GreenStimulus Nature Green</td>\n",
       "      <td>us</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ðŸ‡ºðŸ‡¸ PandÃ©mie de #coronavirus: 30 pasteurs amÃ©ri...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>coronavirus COVID__19 COVIDãƒ¼19</td>\n",
       "      <td>us</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text reply_to_screen_name  \\\n",
       "0  Remember the #WuhanCoronaVirus? The pandemic w...                  NaN   \n",
       "1  My sources @WhiteHouse say 2 tactics will be u...                  NaN   \n",
       "2  I'll venture a wild guess: If you were running...                  NaN   \n",
       "3  #Pakistan (#GreenStimulus = #Nature protection...                  NaN   \n",
       "4  ðŸ‡ºðŸ‡¸ PandÃ©mie de #coronavirus: 30 pasteurs amÃ©ri...                  NaN   \n",
       "\n",
       "  is_quote is_retweet                             hashtags country  \n",
       "0    False       True         WuhanCoronaVirus KillerCuomo      us  \n",
       "1    False       True                                Trump      us  \n",
       "2    False       True                              COVID19      us  \n",
       "3    False       True  Pakistan GreenStimulus Nature Green      us  \n",
       "4    False       True       coronavirus COVID__19 COVIDãƒ¼19      us  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv(\"training_data.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c98f98",
   "metadata": {},
   "source": [
    "##### converting the text column in the dataframe into a list with strings inside and then tokenizing the data\n",
    "\n",
    "###### reference: https://stackoverflow.com/questions/41768196/python-convert-dataframe-into-a-list-with-string-items-inside-list\n",
    "###### reference: https://www.analyticsvidhya.com/blog/2021/07/nltk-a-beginners-hands-on-guide-to-natural-language-processing/#:~:text=NLTK%20is%20a%20toolkit%20build,%2C%20parse%20tree%20visualization%2C%20etc%E2%80%A6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8043d25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "str_data=df['text'].astype(str).values.tolist()\n",
    "token_data = [nltk.word_tokenize(i) for i in str_data]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd498fb",
   "metadata": {},
   "source": [
    "##### storing the tokenized text as another column in the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5793ad93",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['token_data']=token_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd7f64b",
   "metadata": {},
   "source": [
    "##### pos-tagging and lemmatizing the Tokenized data and adding the lemmatized text as a column in the data frame\n",
    "###### Reference: https://stackoverflow.com/questions/15586721/wordnet-lemmatization-and-pos-tagging-in-python\n",
    "###### Reference: https://towardsdatascience.com/building-a-text-normalizer-using-nltk-ft-pos-tagger-e713e611db8\n",
    "###### Reference: https://www.machinelearningplus.com/nlp/lemmatization-examples-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f5dda0fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>reply_to_screen_name</th>\n",
       "      <th>is_quote</th>\n",
       "      <th>is_retweet</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>country</th>\n",
       "      <th>token_data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Remember the #WuhanCoronaVirus? The pandemic w...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>WuhanCoronaVirus KillerCuomo</td>\n",
       "      <td>us</td>\n",
       "      <td>[Remember, the, #, WuhanCoronaVirus, ?, The, p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>My sources @WhiteHouse say 2 tactics will be u...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>Trump</td>\n",
       "      <td>us</td>\n",
       "      <td>[My, sources, @, WhiteHouse, say, 2, tactics, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I'll venture a wild guess: If you were running...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>COVID19</td>\n",
       "      <td>us</td>\n",
       "      <td>[I, 'll, venture, a, wild, guess, :, If, you, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#Pakistan (#GreenStimulus = #Nature protection...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>Pakistan GreenStimulus Nature Green</td>\n",
       "      <td>us</td>\n",
       "      <td>[#, Pakistan, (, #, GreenStimulus, =, #, Natur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ðŸ‡ºðŸ‡¸ PandÃ©mie de #coronavirus: 30 pasteurs amÃ©ri...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>coronavirus COVID__19 COVIDãƒ¼19</td>\n",
       "      <td>us</td>\n",
       "      <td>[ðŸ‡ºðŸ‡¸, PandÃ©mie, de, #, coronavirus, :, 30, past...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text reply_to_screen_name  \\\n",
       "0  Remember the #WuhanCoronaVirus? The pandemic w...                  NaN   \n",
       "1  My sources @WhiteHouse say 2 tactics will be u...                  NaN   \n",
       "2  I'll venture a wild guess: If you were running...                  NaN   \n",
       "3  #Pakistan (#GreenStimulus = #Nature protection...                  NaN   \n",
       "4  ðŸ‡ºðŸ‡¸ PandÃ©mie de #coronavirus: 30 pasteurs amÃ©ri...                  NaN   \n",
       "\n",
       "  is_quote is_retweet                             hashtags country  \\\n",
       "0    False       True         WuhanCoronaVirus KillerCuomo      us   \n",
       "1    False       True                                Trump      us   \n",
       "2    False       True                              COVID19      us   \n",
       "3    False       True  Pakistan GreenStimulus Nature Green      us   \n",
       "4    False       True       coronavirus COVID__19 COVIDãƒ¼19      us   \n",
       "\n",
       "                                          token_data  \n",
       "0  [Remember, the, #, WuhanCoronaVirus, ?, The, p...  \n",
       "1  [My, sources, @, WhiteHouse, say, 2, tactics, ...  \n",
       "2  [I, 'll, venture, a, wild, guess, :, If, you, ...  \n",
       "3  [#, Pakistan, (, #, GreenStimulus, =, #, Natur...  \n",
       "4  [ðŸ‡ºðŸ‡¸, PandÃ©mie, de, #, coronavirus, :, 30, past...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d91c1c3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 240000/240000 [33:05<00:00, 120.88it/s]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus.reader.wordnet import VERB, NOUN, ADJ, ADV\n",
    "lemmatized_data=[]\n",
    "def mapping_pos(w):\n",
    "    t = nltk.pos_tag([w])[0][1][0].upper()\n",
    "    dict_pos_map = {\n",
    "        # Look for N in the POS tag because all nouns begin with N\n",
    "        'N': NOUN,\n",
    "        # Look for V in the POS tag because all nouns begin with V\n",
    "        'V':VERB,\n",
    "        # Look for J in the POS tag because all nouns begin with J\n",
    "        'J' : ADJ,\n",
    "        # Look for R in the POS tag because all nouns begin with R\n",
    "        'R':ADV  \n",
    "    }\n",
    "    return dict_pos_map.get(t, NOUN)\n",
    "\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lmtzr = WordNetLemmatizer()\n",
    "for i in tqdm(range(0, len(str_data)),desc=\"Progress\"):\n",
    "    x=[lmtzr.lemmatize(w, mapping_pos(w)) for w in nltk.word_tokenize(str_data[i])]\n",
    "    lemmatized_data.append(x)\n",
    "    \n",
    "df['lemmatized_data']=lemmatized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bc08361a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>reply_to_screen_name</th>\n",
       "      <th>is_quote</th>\n",
       "      <th>is_retweet</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>country</th>\n",
       "      <th>token_data</th>\n",
       "      <th>lemmatized_data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Remember the #WuhanCoronaVirus? The pandemic w...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>WuhanCoronaVirus KillerCuomo</td>\n",
       "      <td>us</td>\n",
       "      <td>[Remember, the, #, WuhanCoronaVirus, ?, The, p...</td>\n",
       "      <td>[Remember, the, #, WuhanCoronaVirus, ?, The, p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>My sources @WhiteHouse say 2 tactics will be u...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>Trump</td>\n",
       "      <td>us</td>\n",
       "      <td>[My, sources, @, WhiteHouse, say, 2, tactics, ...</td>\n",
       "      <td>[My, source, @, WhiteHouse, say, 2, tactic, wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I'll venture a wild guess: If you were running...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>COVID19</td>\n",
       "      <td>us</td>\n",
       "      <td>[I, 'll, venture, a, wild, guess, :, If, you, ...</td>\n",
       "      <td>[I, 'll, venture, a, wild, guess, :, If, you, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#Pakistan (#GreenStimulus = #Nature protection...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>Pakistan GreenStimulus Nature Green</td>\n",
       "      <td>us</td>\n",
       "      <td>[#, Pakistan, (, #, GreenStimulus, =, #, Natur...</td>\n",
       "      <td>[#, Pakistan, (, #, GreenStimulus, =, #, Natur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ðŸ‡ºðŸ‡¸ PandÃ©mie de #coronavirus: 30 pasteurs amÃ©ri...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>coronavirus COVID__19 COVIDãƒ¼19</td>\n",
       "      <td>us</td>\n",
       "      <td>[ðŸ‡ºðŸ‡¸, PandÃ©mie, de, #, coronavirus, :, 30, past...</td>\n",
       "      <td>[ðŸ‡ºðŸ‡¸, PandÃ©mie, de, #, coronavirus, :, 30, past...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text reply_to_screen_name  \\\n",
       "0  Remember the #WuhanCoronaVirus? The pandemic w...                  NaN   \n",
       "1  My sources @WhiteHouse say 2 tactics will be u...                  NaN   \n",
       "2  I'll venture a wild guess: If you were running...                  NaN   \n",
       "3  #Pakistan (#GreenStimulus = #Nature protection...                  NaN   \n",
       "4  ðŸ‡ºðŸ‡¸ PandÃ©mie de #coronavirus: 30 pasteurs amÃ©ri...                  NaN   \n",
       "\n",
       "  is_quote is_retweet                             hashtags country  \\\n",
       "0    False       True         WuhanCoronaVirus KillerCuomo      us   \n",
       "1    False       True                                Trump      us   \n",
       "2    False       True                              COVID19      us   \n",
       "3    False       True  Pakistan GreenStimulus Nature Green      us   \n",
       "4    False       True       coronavirus COVID__19 COVIDãƒ¼19      us   \n",
       "\n",
       "                                          token_data  \\\n",
       "0  [Remember, the, #, WuhanCoronaVirus, ?, The, p...   \n",
       "1  [My, sources, @, WhiteHouse, say, 2, tactics, ...   \n",
       "2  [I, 'll, venture, a, wild, guess, :, If, you, ...   \n",
       "3  [#, Pakistan, (, #, GreenStimulus, =, #, Natur...   \n",
       "4  [ðŸ‡ºðŸ‡¸, PandÃ©mie, de, #, coronavirus, :, 30, past...   \n",
       "\n",
       "                                     lemmatized_data  \n",
       "0  [Remember, the, #, WuhanCoronaVirus, ?, The, p...  \n",
       "1  [My, source, @, WhiteHouse, say, 2, tactic, wi...  \n",
       "2  [I, 'll, venture, a, wild, guess, :, If, you, ...  \n",
       "3  [#, Pakistan, (, #, GreenStimulus, =, #, Natur...  \n",
       "4  [ðŸ‡ºðŸ‡¸, PandÃ©mie, de, #, coronavirus, :, 30, past...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f7e06533",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('cleaned_lemmatized_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1f5133",
   "metadata": {},
   "source": [
    "##### removing the stopwords from the lemmatized text and adding filtered sentence in a column to the dataframe \n",
    "###### reference: https://www.geeksforgeeks.org/removing-stop-words-nltk-python/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c51ab106",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 240000/240000 [00:01<00:00, 158055.04it/s]\n"
     ]
    }
   ],
   "source": [
    "filtered_sentence = []\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "for i in tqdm(range(0,len(lemmatized_data)),desc=\"Progress\"):\n",
    "    sentence = lemmatized_data[i]\n",
    "    filtered_sentence.append([w for w in sentence if not w in stop_words])\n",
    "\n",
    "df['filtered_sentence']= filtered_sentence    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "77b3537e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>reply_to_screen_name</th>\n",
       "      <th>is_quote</th>\n",
       "      <th>is_retweet</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>country</th>\n",
       "      <th>token_data</th>\n",
       "      <th>lemmatized_data</th>\n",
       "      <th>filtered_sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Remember the #WuhanCoronaVirus? The pandemic w...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>WuhanCoronaVirus KillerCuomo</td>\n",
       "      <td>us</td>\n",
       "      <td>[Remember, the, #, WuhanCoronaVirus, ?, The, p...</td>\n",
       "      <td>[Remember, the, #, WuhanCoronaVirus, ?, The, p...</td>\n",
       "      <td>[Remember, #, WuhanCoronaVirus, ?, The, pandem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>My sources @WhiteHouse say 2 tactics will be u...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>Trump</td>\n",
       "      <td>us</td>\n",
       "      <td>[My, sources, @, WhiteHouse, say, 2, tactics, ...</td>\n",
       "      <td>[My, source, @, WhiteHouse, say, 2, tactic, wi...</td>\n",
       "      <td>[My, source, @, WhiteHouse, say, 2, tactic, us...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I'll venture a wild guess: If you were running...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>COVID19</td>\n",
       "      <td>us</td>\n",
       "      <td>[I, 'll, venture, a, wild, guess, :, If, you, ...</td>\n",
       "      <td>[I, 'll, venture, a, wild, guess, :, If, you, ...</td>\n",
       "      <td>[I, 'll, venture, wild, guess, :, If, run, USA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#Pakistan (#GreenStimulus = #Nature protection...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>Pakistan GreenStimulus Nature Green</td>\n",
       "      <td>us</td>\n",
       "      <td>[#, Pakistan, (, #, GreenStimulus, =, #, Natur...</td>\n",
       "      <td>[#, Pakistan, (, #, GreenStimulus, =, #, Natur...</td>\n",
       "      <td>[#, Pakistan, (, #, GreenStimulus, =, #, Natur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ðŸ‡ºðŸ‡¸ PandÃ©mie de #coronavirus: 30 pasteurs amÃ©ri...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>coronavirus COVID__19 COVIDãƒ¼19</td>\n",
       "      <td>us</td>\n",
       "      <td>[ðŸ‡ºðŸ‡¸, PandÃ©mie, de, #, coronavirus, :, 30, past...</td>\n",
       "      <td>[ðŸ‡ºðŸ‡¸, PandÃ©mie, de, #, coronavirus, :, 30, past...</td>\n",
       "      <td>[ðŸ‡ºðŸ‡¸, PandÃ©mie, de, #, coronavirus, :, 30, past...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text reply_to_screen_name  \\\n",
       "0  Remember the #WuhanCoronaVirus? The pandemic w...                  NaN   \n",
       "1  My sources @WhiteHouse say 2 tactics will be u...                  NaN   \n",
       "2  I'll venture a wild guess: If you were running...                  NaN   \n",
       "3  #Pakistan (#GreenStimulus = #Nature protection...                  NaN   \n",
       "4  ðŸ‡ºðŸ‡¸ PandÃ©mie de #coronavirus: 30 pasteurs amÃ©ri...                  NaN   \n",
       "\n",
       "  is_quote is_retweet                             hashtags country  \\\n",
       "0    False       True         WuhanCoronaVirus KillerCuomo      us   \n",
       "1    False       True                                Trump      us   \n",
       "2    False       True                              COVID19      us   \n",
       "3    False       True  Pakistan GreenStimulus Nature Green      us   \n",
       "4    False       True       coronavirus COVID__19 COVIDãƒ¼19      us   \n",
       "\n",
       "                                          token_data  \\\n",
       "0  [Remember, the, #, WuhanCoronaVirus, ?, The, p...   \n",
       "1  [My, sources, @, WhiteHouse, say, 2, tactics, ...   \n",
       "2  [I, 'll, venture, a, wild, guess, :, If, you, ...   \n",
       "3  [#, Pakistan, (, #, GreenStimulus, =, #, Natur...   \n",
       "4  [ðŸ‡ºðŸ‡¸, PandÃ©mie, de, #, coronavirus, :, 30, past...   \n",
       "\n",
       "                                     lemmatized_data  \\\n",
       "0  [Remember, the, #, WuhanCoronaVirus, ?, The, p...   \n",
       "1  [My, source, @, WhiteHouse, say, 2, tactic, wi...   \n",
       "2  [I, 'll, venture, a, wild, guess, :, If, you, ...   \n",
       "3  [#, Pakistan, (, #, GreenStimulus, =, #, Natur...   \n",
       "4  [ðŸ‡ºðŸ‡¸, PandÃ©mie, de, #, coronavirus, :, 30, past...   \n",
       "\n",
       "                                   filtered_sentence  \n",
       "0  [Remember, #, WuhanCoronaVirus, ?, The, pandem...  \n",
       "1  [My, source, @, WhiteHouse, say, 2, tactic, us...  \n",
       "2  [I, 'll, venture, wild, guess, :, If, run, USA...  \n",
       "3  [#, Pakistan, (, #, GreenStimulus, =, #, Natur...  \n",
       "4  [ðŸ‡ºðŸ‡¸, PandÃ©mie, de, #, coronavirus, :, 30, past...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ae3ca3",
   "metadata": {},
   "source": [
    "##### Removing numbers, words shorter than 2 chars, punctuations, links and emojis\n",
    "##### Then joining the resulting cleaning text into a joined string (joined by space ' ')\n",
    "###### reference: https://docs.python.org/3/howto/regex.html\n",
    "###### reference: https://gist.github.com/slowkow/7a7f61f495e3dbb7e3d767f97bd7304b?permalink_comment_id=3203132\n",
    "###### reference: https://stackoverflow.com/questions/34293875/how-to-remove-punctuation-marks-from-a-string-in-python-3-x-using-translate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c72075be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 240000/240000 [00:09<00:00, 25699.59it/s]\n"
     ]
    }
   ],
   "source": [
    "clean_t = (df['filtered_sentence']).astype(str).to_list()\n",
    "text_clean=[]\n",
    "\n",
    "emoji = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                               u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U000024C2-\\U0001F251\"\n",
    "                               u\"\\U0001f926-\\U0001f937\"\n",
    "                               u\"\\U00010000-\\U0010ffff\"\n",
    "                               u\"\\u2640-\\u2642\"\n",
    "                               u\"\\u2600-\\u2B55\"\n",
    "                               u\"\\u200d\"\n",
    "                               u\"\\u23cf\"\n",
    "                               u\"\\u23e9\"\n",
    "                               u\"\\u231a\"\n",
    "                               u\"\\ufe0f\"  # dingbats\n",
    "                               u\"\\u3030\"\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "\n",
    "puncts = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', 'â€¢',  '~', '@', 'Â£',\n",
    " 'Â·', '_', '{', '}', 'Â©', '^', 'Â®', '`',  '<', 'â†’', 'Â°', 'â‚¬', 'â„¢', 'â€º',  'â™¥', 'â†', 'Ã—', 'Â§', 'â€³', 'â€²', 'Ã‚', 'â–ˆ', 'Â½', 'Ã ', 'â€¦',\n",
    " 'â€œ', 'â˜…', 'â€', 'â€“', 'â—', 'Ã¢', 'â–º', 'âˆ’', 'Â¢', 'Â²', 'Â¬', 'â–‘', 'Â¶', 'â†‘', 'Â±', 'Â¿', 'â–¾', 'â•', 'Â¦', 'â•‘', 'â€•', 'Â¥', 'â–“', 'â€”', 'â€¹', 'â”€',\n",
    " 'â–’', 'ï¼š', 'Â¼', 'âŠ•', 'â–¼', 'â–ª', 'â€ ', 'â– ', 'â€™', 'â–€', 'Â¨', 'â–„', 'â™«', 'â˜†', 'Ã©', 'Â¯', 'â™¦', 'Â¤', 'â–²', 'Ã¨', 'Â¸', 'Â¾', 'Ãƒ', 'â‹…', 'â€˜', 'âˆž',\n",
    " 'âˆ™', 'ï¼‰', 'â†“', 'ã€', 'â”‚', 'ï¼ˆ', 'Â»', 'ï¼Œ', 'â™ª', 'â•©', 'â•š', 'Â³', 'ãƒ»', 'â•¦', 'â•£', 'â•”', 'â•—', 'â–¬', 'â¤', 'Ã¯', 'Ã˜', 'Â¹', 'â‰¤', 'â€¡', 'âˆš', ]\n",
    "\n",
    "\n",
    "for i in tqdm(range(0,len(filtered_sentence)),desc=\"Progress\"):\n",
    "    #removing words with characters less than 2\n",
    "    clean=re.sub(r'\\b\\w{1,2}\\b',\"\",clean_t[i])\n",
    "    #removing punctuations\n",
    "    clean=re.sub(r'[^\\w\\s]', '',clean)\n",
    "    #removing numbers\n",
    "    clean=re.sub(r'\\d+', '', clean)\n",
    "    #removing links\n",
    "    clean=re.sub(r'http\\S+', '', clean)   \n",
    "    #removing emojis\n",
    "    clean= emoji.sub(r'', clean)\n",
    "    for punct in puncts:\n",
    "            if punct in clean:\n",
    "                    clean = clean.replace(punct,'')\n",
    "        \n",
    "    #joining the list of words back inot a joined string joined by space\n",
    "    text_clean.append(clean)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b375c450",
   "metadata": {},
   "source": [
    "##### adding the resulting clean text to the data frame as column text_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "183bb24a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>reply_to_screen_name</th>\n",
       "      <th>is_quote</th>\n",
       "      <th>is_retweet</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>country</th>\n",
       "      <th>token_data</th>\n",
       "      <th>lemmatized_data</th>\n",
       "      <th>filtered_sentence</th>\n",
       "      <th>text_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Remember the #WuhanCoronaVirus? The pandemic w...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>WuhanCoronaVirus KillerCuomo</td>\n",
       "      <td>us</td>\n",
       "      <td>[Remember, the, #, WuhanCoronaVirus, ?, The, p...</td>\n",
       "      <td>[Remember, the, #, WuhanCoronaVirus, ?, The, p...</td>\n",
       "      <td>[Remember, #, WuhanCoronaVirus, ?, The, pandem...</td>\n",
       "      <td>Remember  WuhanCoronaVirus  The pandemic great...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>My sources @WhiteHouse say 2 tactics will be u...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>Trump</td>\n",
       "      <td>us</td>\n",
       "      <td>[My, sources, @, WhiteHouse, say, 2, tactics, ...</td>\n",
       "      <td>[My, source, @, WhiteHouse, say, 2, tactic, wi...</td>\n",
       "      <td>[My, source, @, WhiteHouse, say, 2, tactic, us...</td>\n",
       "      <td>source  WhiteHouse say  tactic use get Americ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I'll venture a wild guess: If you were running...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>COVID19</td>\n",
       "      <td>us</td>\n",
       "      <td>[I, 'll, venture, a, wild, guess, :, If, you, ...</td>\n",
       "      <td>[I, 'll, venture, a, wild, guess, :, If, you, ...</td>\n",
       "      <td>[I, 'll, venture, wild, guess, :, If, run, USA...</td>\n",
       "      <td>venture wild guess   run USA  COVID crisis  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#Pakistan (#GreenStimulus = #Nature protection...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>Pakistan GreenStimulus Nature Green</td>\n",
       "      <td>us</td>\n",
       "      <td>[#, Pakistan, (, #, GreenStimulus, =, #, Natur...</td>\n",
       "      <td>[#, Pakistan, (, #, GreenStimulus, =, #, Natur...</td>\n",
       "      <td>[#, Pakistan, (, #, GreenStimulus, =, #, Natur...</td>\n",
       "      <td>Pakistan   GreenStimulus   Nature protection ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ðŸ‡ºðŸ‡¸ PandÃ©mie de #coronavirus: 30 pasteurs amÃ©ri...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>coronavirus COVID__19 COVIDãƒ¼19</td>\n",
       "      <td>us</td>\n",
       "      <td>[ðŸ‡ºðŸ‡¸, PandÃ©mie, de, #, coronavirus, :, 30, past...</td>\n",
       "      <td>[ðŸ‡ºðŸ‡¸, PandÃ©mie, de, #, coronavirus, :, 30, past...</td>\n",
       "      <td>[ðŸ‡ºðŸ‡¸, PandÃ©mie, de, #, coronavirus, :, 30, past...</td>\n",
       "      <td>Pandmie   coronavirus   pasteur amricains qui...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text reply_to_screen_name  \\\n",
       "0  Remember the #WuhanCoronaVirus? The pandemic w...                  NaN   \n",
       "1  My sources @WhiteHouse say 2 tactics will be u...                  NaN   \n",
       "2  I'll venture a wild guess: If you were running...                  NaN   \n",
       "3  #Pakistan (#GreenStimulus = #Nature protection...                  NaN   \n",
       "4  ðŸ‡ºðŸ‡¸ PandÃ©mie de #coronavirus: 30 pasteurs amÃ©ri...                  NaN   \n",
       "\n",
       "  is_quote is_retweet                             hashtags country  \\\n",
       "0    False       True         WuhanCoronaVirus KillerCuomo      us   \n",
       "1    False       True                                Trump      us   \n",
       "2    False       True                              COVID19      us   \n",
       "3    False       True  Pakistan GreenStimulus Nature Green      us   \n",
       "4    False       True       coronavirus COVID__19 COVIDãƒ¼19      us   \n",
       "\n",
       "                                          token_data  \\\n",
       "0  [Remember, the, #, WuhanCoronaVirus, ?, The, p...   \n",
       "1  [My, sources, @, WhiteHouse, say, 2, tactics, ...   \n",
       "2  [I, 'll, venture, a, wild, guess, :, If, you, ...   \n",
       "3  [#, Pakistan, (, #, GreenStimulus, =, #, Natur...   \n",
       "4  [ðŸ‡ºðŸ‡¸, PandÃ©mie, de, #, coronavirus, :, 30, past...   \n",
       "\n",
       "                                     lemmatized_data  \\\n",
       "0  [Remember, the, #, WuhanCoronaVirus, ?, The, p...   \n",
       "1  [My, source, @, WhiteHouse, say, 2, tactic, wi...   \n",
       "2  [I, 'll, venture, a, wild, guess, :, If, you, ...   \n",
       "3  [#, Pakistan, (, #, GreenStimulus, =, #, Natur...   \n",
       "4  [ðŸ‡ºðŸ‡¸, PandÃ©mie, de, #, coronavirus, :, 30, past...   \n",
       "\n",
       "                                   filtered_sentence  \\\n",
       "0  [Remember, #, WuhanCoronaVirus, ?, The, pandem...   \n",
       "1  [My, source, @, WhiteHouse, say, 2, tactic, us...   \n",
       "2  [I, 'll, venture, wild, guess, :, If, run, USA...   \n",
       "3  [#, Pakistan, (, #, GreenStimulus, =, #, Natur...   \n",
       "4  [ðŸ‡ºðŸ‡¸, PandÃ©mie, de, #, coronavirus, :, 30, past...   \n",
       "\n",
       "                                          text_clean  \n",
       "0  Remember  WuhanCoronaVirus  The pandemic great...  \n",
       "1   source  WhiteHouse say  tactic use get Americ...  \n",
       "2    venture wild guess   run USA  COVID crisis  ...  \n",
       "3   Pakistan   GreenStimulus   Nature protection ...  \n",
       "4   Pandmie   coronavirus   pasteur amricains qui...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text_clean']=text_clean\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8fb8711f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('cleaned_final_textCleaned_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540a0cc6",
   "metadata": {},
   "source": [
    "### Training the  Base Line Model using Logistic Regression and Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6fe0f227",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>reply_to_screen_name</th>\n",
       "      <th>is_quote</th>\n",
       "      <th>is_retweet</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>country</th>\n",
       "      <th>token_data</th>\n",
       "      <th>lemmatized_data</th>\n",
       "      <th>filtered_sentence</th>\n",
       "      <th>text_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Remember the #WuhanCoronaVirus? The pandemic w...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>WuhanCoronaVirus KillerCuomo</td>\n",
       "      <td>us</td>\n",
       "      <td>['Remember', 'the', '#', 'WuhanCoronaVirus', '...</td>\n",
       "      <td>['Remember', 'the', '#', 'WuhanCoronaVirus', '...</td>\n",
       "      <td>['Remember', '#', 'WuhanCoronaVirus', '?', 'Th...</td>\n",
       "      <td>Remember  WuhanCoronaVirus  The pandemic great...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>My sources @WhiteHouse say 2 tactics will be u...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>Trump</td>\n",
       "      <td>us</td>\n",
       "      <td>['My', 'sources', '@', 'WhiteHouse', 'say', '2...</td>\n",
       "      <td>['My', 'source', '@', 'WhiteHouse', 'say', '2'...</td>\n",
       "      <td>['My', 'source', '@', 'WhiteHouse', 'say', '2'...</td>\n",
       "      <td>source  WhiteHouse say  tactic use get Americ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I'll venture a wild guess: If you were running...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>COVID19</td>\n",
       "      <td>us</td>\n",
       "      <td>['I', \"'ll\", 'venture', 'a', 'wild', 'guess', ...</td>\n",
       "      <td>['I', \"'ll\", 'venture', 'a', 'wild', 'guess', ...</td>\n",
       "      <td>['I', \"'ll\", 'venture', 'wild', 'guess', ':', ...</td>\n",
       "      <td>venture wild guess   run USA  COVID crisis  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#Pakistan (#GreenStimulus = #Nature protection...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>Pakistan GreenStimulus Nature Green</td>\n",
       "      <td>us</td>\n",
       "      <td>['#', 'Pakistan', '(', '#', 'GreenStimulus', '...</td>\n",
       "      <td>['#', 'Pakistan', '(', '#', 'GreenStimulus', '...</td>\n",
       "      <td>['#', 'Pakistan', '(', '#', 'GreenStimulus', '...</td>\n",
       "      <td>Pakistan   GreenStimulus   Nature protection ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ðŸ‡ºðŸ‡¸ PandÃ©mie de #coronavirus: 30 pasteurs amÃ©ri...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>coronavirus COVID__19 COVIDãƒ¼19</td>\n",
       "      <td>us</td>\n",
       "      <td>['ðŸ‡ºðŸ‡¸', 'PandÃ©mie', 'de', '#', 'coronavirus', '...</td>\n",
       "      <td>['ðŸ‡ºðŸ‡¸', 'PandÃ©mie', 'de', '#', 'coronavirus', '...</td>\n",
       "      <td>['ðŸ‡ºðŸ‡¸', 'PandÃ©mie', 'de', '#', 'coronavirus', '...</td>\n",
       "      <td>Pandmie   coronavirus   pasteur amricains qui...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text reply_to_screen_name  \\\n",
       "0  Remember the #WuhanCoronaVirus? The pandemic w...                  NaN   \n",
       "1  My sources @WhiteHouse say 2 tactics will be u...                  NaN   \n",
       "2  I'll venture a wild guess: If you were running...                  NaN   \n",
       "3  #Pakistan (#GreenStimulus = #Nature protection...                  NaN   \n",
       "4  ðŸ‡ºðŸ‡¸ PandÃ©mie de #coronavirus: 30 pasteurs amÃ©ri...                  NaN   \n",
       "\n",
       "  is_quote is_retweet                             hashtags country  \\\n",
       "0    False       True         WuhanCoronaVirus KillerCuomo      us   \n",
       "1    False       True                                Trump      us   \n",
       "2    False       True                              COVID19      us   \n",
       "3    False       True  Pakistan GreenStimulus Nature Green      us   \n",
       "4    False       True       coronavirus COVID__19 COVIDãƒ¼19      us   \n",
       "\n",
       "                                          token_data  \\\n",
       "0  ['Remember', 'the', '#', 'WuhanCoronaVirus', '...   \n",
       "1  ['My', 'sources', '@', 'WhiteHouse', 'say', '2...   \n",
       "2  ['I', \"'ll\", 'venture', 'a', 'wild', 'guess', ...   \n",
       "3  ['#', 'Pakistan', '(', '#', 'GreenStimulus', '...   \n",
       "4  ['ðŸ‡ºðŸ‡¸', 'PandÃ©mie', 'de', '#', 'coronavirus', '...   \n",
       "\n",
       "                                     lemmatized_data  \\\n",
       "0  ['Remember', 'the', '#', 'WuhanCoronaVirus', '...   \n",
       "1  ['My', 'source', '@', 'WhiteHouse', 'say', '2'...   \n",
       "2  ['I', \"'ll\", 'venture', 'a', 'wild', 'guess', ...   \n",
       "3  ['#', 'Pakistan', '(', '#', 'GreenStimulus', '...   \n",
       "4  ['ðŸ‡ºðŸ‡¸', 'PandÃ©mie', 'de', '#', 'coronavirus', '...   \n",
       "\n",
       "                                   filtered_sentence  \\\n",
       "0  ['Remember', '#', 'WuhanCoronaVirus', '?', 'Th...   \n",
       "1  ['My', 'source', '@', 'WhiteHouse', 'say', '2'...   \n",
       "2  ['I', \"'ll\", 'venture', 'wild', 'guess', ':', ...   \n",
       "3  ['#', 'Pakistan', '(', '#', 'GreenStimulus', '...   \n",
       "4  ['ðŸ‡ºðŸ‡¸', 'PandÃ©mie', 'de', '#', 'coronavirus', '...   \n",
       "\n",
       "                                          text_clean  \n",
       "0  Remember  WuhanCoronaVirus  The pandemic great...  \n",
       "1   source  WhiteHouse say  tactic use get Americ...  \n",
       "2    venture wild guess   run USA  COVID crisis  ...  \n",
       "3   Pakistan   GreenStimulus   Nature protection ...  \n",
       "4   Pandmie   coronavirus   pasteur amricains qui...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv(\"cleaned_final_textCleaned_data.csv\",low_memory=False)\n",
    "df=df.drop(columns=[\"Unnamed: 0\"])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "220bfe82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         Remember  WuhanCoronaVirus  The pandemic great...\n",
       "1          source  WhiteHouse say  tactic use get Americ...\n",
       "2           venture wild guess   run USA  COVID crisis  ...\n",
       "3          Pakistan   GreenStimulus   Nature protection ...\n",
       "4          Pandmie   coronavirus   pasteur amricains qui...\n",
       "                                ...                        \n",
       "239995                     Likes  Retweets yentra    MastÐµr\n",
       "239996    Very interest Any thought   TheFive  Trump  KA...\n",
       "239997     deal  COVID  forget  Christians   persecution...\n",
       "239998    While hit   COVID death  President golf First ...\n",
       "239999    This shall pas  Covid  May remain stand midst ...\n",
       "Name: text_clean, Length: 240000, dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text_clean']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb599814",
   "metadata": {},
   "source": [
    "#### splitting the Training and Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "759bee02",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"text_clean\"]=df[\"text_clean\"].values.astype('U')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ec8f3de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df[\"text_clean\"],df[\"country\"],test_size=0.2,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f3b73913",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1_train = TfidfVectorizer(use_idf=True)\n",
    "X_t1_train = t1_train.fit_transform(X_train)\n",
    "X_t1_test = t1_train.transform(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e35961b",
   "metadata": {},
   "source": [
    "### LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "07fa9316",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "   australia       0.48      0.44      0.46      7956\n",
      "      canada       0.36      0.34      0.35      7955\n",
      "     ireland       0.62      0.62      0.62      7999\n",
      " new_zealand       0.45      0.40      0.42      8023\n",
      "          uk       0.33      0.34      0.33      8102\n",
      "          us       0.41      0.52      0.46      7965\n",
      "\n",
      "    accuracy                           0.44     48000\n",
      "   macro avg       0.44      0.44      0.44     48000\n",
      "weighted avg       0.44      0.44      0.44     48000\n",
      "\n",
      "Confusion Matrix: [[3465  909  440  792  873 1477]\n",
      " [ 940 2696  689  989 1674  967]\n",
      " [ 371  617 4987  459  996  569]\n",
      " [ 729 1065  518 3185  962 1564]\n",
      " [ 813 1417  988  777 2723 1384]\n",
      " [ 861  690  407  899  959 4149]]\n"
     ]
    }
   ],
   "source": [
    "LR=LogisticRegression(solver = 'liblinear', C=10, penalty = 'l2')\n",
    "LR.fit(X_t1_train, y_train)  #Predict y value for test dataset\n",
    "\n",
    "y_predict = LR.predict(X_t1_test)\n",
    "y_prob = LR.predict_proba(X_t1_test)[:,1]\n",
    "\n",
    "print(classification_report(y_test,y_predict))\n",
    "print('Confusion Matrix:',confusion_matrix(y_test, y_predict))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a139f0",
   "metadata": {},
   "source": [
    "### Naive Bayes MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b6974362",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB,MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e46b62af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "   australia       0.54      0.41      0.47      7956\n",
      "      canada       0.41      0.29      0.34      7955\n",
      "     ireland       0.57      0.65      0.60      7999\n",
      " new_zealand       0.43      0.40      0.41      8023\n",
      "          uk       0.35      0.34      0.35      8102\n",
      "          us       0.41      0.62      0.49      7965\n",
      "\n",
      "    accuracy                           0.45     48000\n",
      "   macro avg       0.45      0.45      0.44     48000\n",
      "weighted avg       0.45      0.45      0.44     48000\n",
      "\n",
      "Confusion Matrix: [[3262  645  621  904  743 1781]\n",
      " [ 744 2304  942 1141 1717 1107]\n",
      " [ 286  390 5175  571  883  694]\n",
      " [ 539  781  672 3223  914 1894]\n",
      " [ 633 1015 1268  865 2728 1593]\n",
      " [ 571  461  454  854  708 4917]]\n"
     ]
    }
   ],
   "source": [
    "gnb = MultinomialNB(alpha=0.7)\n",
    "gnb.fit(X_t1_train, y_train)\n",
    "y_predict = gnb.predict(X_t1_test)\n",
    "y_prob = gnb.predict_proba(X_t1_test)[:,1]\n",
    "\n",
    "print(classification_report(y_test,y_predict))\n",
    "print('Confusion Matrix:',confusion_matrix(y_test, y_predict))     \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc974d9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ce8b5c21",
   "metadata": {},
   "source": [
    "## Training a LSTM Model (Final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d99faf12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>reply_to_screen_name</th>\n",
       "      <th>is_quote</th>\n",
       "      <th>is_retweet</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>country</th>\n",
       "      <th>token_data</th>\n",
       "      <th>lemmatized_data</th>\n",
       "      <th>filtered_sentence</th>\n",
       "      <th>text_clean</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index_no</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Remember the #WuhanCoronaVirus? The pandemic w...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>WuhanCoronaVirus KillerCuomo</td>\n",
       "      <td>us</td>\n",
       "      <td>['Remember', 'the', '#', 'WuhanCoronaVirus', '...</td>\n",
       "      <td>['Remember', 'the', '#', 'WuhanCoronaVirus', '...</td>\n",
       "      <td>['Remember', '#', 'WuhanCoronaVirus', '?', 'Th...</td>\n",
       "      <td>Remember  WuhanCoronaVirus  The pandemic great...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>My sources @WhiteHouse say 2 tactics will be u...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>Trump</td>\n",
       "      <td>us</td>\n",
       "      <td>['My', 'sources', '@', 'WhiteHouse', 'say', '2...</td>\n",
       "      <td>['My', 'source', '@', 'WhiteHouse', 'say', '2'...</td>\n",
       "      <td>['My', 'source', '@', 'WhiteHouse', 'say', '2'...</td>\n",
       "      <td>source  WhiteHouse say  tactic use get Americ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I'll venture a wild guess: If you were running...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>COVID19</td>\n",
       "      <td>us</td>\n",
       "      <td>['I', \"'ll\", 'venture', 'a', 'wild', 'guess', ...</td>\n",
       "      <td>['I', \"'ll\", 'venture', 'a', 'wild', 'guess', ...</td>\n",
       "      <td>['I', \"'ll\", 'venture', 'wild', 'guess', ':', ...</td>\n",
       "      <td>venture wild guess   run USA  COVID crisis  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#Pakistan (#GreenStimulus = #Nature protection...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>Pakistan GreenStimulus Nature Green</td>\n",
       "      <td>us</td>\n",
       "      <td>['#', 'Pakistan', '(', '#', 'GreenStimulus', '...</td>\n",
       "      <td>['#', 'Pakistan', '(', '#', 'GreenStimulus', '...</td>\n",
       "      <td>['#', 'Pakistan', '(', '#', 'GreenStimulus', '...</td>\n",
       "      <td>Pakistan   GreenStimulus   Nature protection ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ðŸ‡ºðŸ‡¸ PandÃ©mie de #coronavirus: 30 pasteurs amÃ©ri...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>coronavirus COVID__19 COVIDãƒ¼19</td>\n",
       "      <td>us</td>\n",
       "      <td>['ðŸ‡ºðŸ‡¸', 'PandÃ©mie', 'de', '#', 'coronavirus', '...</td>\n",
       "      <td>['ðŸ‡ºðŸ‡¸', 'PandÃ©mie', 'de', '#', 'coronavirus', '...</td>\n",
       "      <td>['ðŸ‡ºðŸ‡¸', 'PandÃ©mie', 'de', '#', 'coronavirus', '...</td>\n",
       "      <td>Pandmie   coronavirus   pasteur amricains qui...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239995</th>\n",
       "      <td>Aa Likes, Retweets yentra ðŸ™\\nðŸ”¥ðŸ”¥ðŸ”¥\\n#MastÐµr</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>MastÐµr</td>\n",
       "      <td>new_zealand</td>\n",
       "      <td>['Aa', 'Likes', ',', 'Retweets', 'yentra', 'ðŸ™'...</td>\n",
       "      <td>['Aa', 'Likes', ',', 'Retweets', 'yentra', 'ðŸ™'...</td>\n",
       "      <td>['Aa', 'Likes', ',', 'Retweets', 'yentra', 'ðŸ™'...</td>\n",
       "      <td>Likes  Retweets yentra    MastÐµr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239996</th>\n",
       "      <td>Very interesting\\nAny thoughts?\\n\\n#TheFive #T...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>FALSE</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>TheFive Trump2020 KAG2020 mondaythoughts COVID...</td>\n",
       "      <td>new_zealand</td>\n",
       "      <td>['Very', 'interesting', 'Any', 'thoughts', '?'...</td>\n",
       "      <td>['Very', 'interest', 'Any', 'thought', '?', '#...</td>\n",
       "      <td>['Very', 'interest', 'Any', 'thought', '?', '#...</td>\n",
       "      <td>Very interest Any thought   TheFive  Trump  KA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239997</th>\n",
       "      <td>As we deal with #COVID19 don't forget that #Ch...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>COVID19 Christians persecution Nigeria</td>\n",
       "      <td>new_zealand</td>\n",
       "      <td>['As', 'we', 'deal', 'with', '#', 'COVID19', '...</td>\n",
       "      <td>['As', 'we', 'deal', 'with', '#', 'COVID19', '...</td>\n",
       "      <td>['As', 'deal', '#', 'COVID19', \"n't\", 'forget'...</td>\n",
       "      <td>deal  COVID  forget  Christians   persecution...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239998</th>\n",
       "      <td>While we hit 150,000 in #COVID19 deaths, the P...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>FALSE</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>COVID19</td>\n",
       "      <td>new_zealand</td>\n",
       "      <td>['While', 'we', 'hit', '150,000', 'in', '#', '...</td>\n",
       "      <td>['While', 'we', 'hit', '150,000', 'in', '#', '...</td>\n",
       "      <td>['While', 'hit', '150,000', '#', 'COVID19', 'd...</td>\n",
       "      <td>While hit   COVID death  President golf First ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239999</th>\n",
       "      <td>This too shall pass #Covid_19 . May remain sta...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>FALSE</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>Covid_19 HopeAlive</td>\n",
       "      <td>new_zealand</td>\n",
       "      <td>['This', 'too', 'shall', 'pass', '#', 'Covid_1...</td>\n",
       "      <td>['This', 'too', 'shall', 'pas', '#', 'Covid_19...</td>\n",
       "      <td>['This', 'shall', 'pas', '#', 'Covid_19', '.',...</td>\n",
       "      <td>This shall pas  Covid  May remain stand midst ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>240000 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       text  \\\n",
       "index_no                                                      \n",
       "0         Remember the #WuhanCoronaVirus? The pandemic w...   \n",
       "1         My sources @WhiteHouse say 2 tactics will be u...   \n",
       "2         I'll venture a wild guess: If you were running...   \n",
       "3         #Pakistan (#GreenStimulus = #Nature protection...   \n",
       "4         ðŸ‡ºðŸ‡¸ PandÃ©mie de #coronavirus: 30 pasteurs amÃ©ri...   \n",
       "...                                                     ...   \n",
       "239995            Aa Likes, Retweets yentra ðŸ™\\nðŸ”¥ðŸ”¥ðŸ”¥\\n#MastÐµr   \n",
       "239996    Very interesting\\nAny thoughts?\\n\\n#TheFive #T...   \n",
       "239997    As we deal with #COVID19 don't forget that #Ch...   \n",
       "239998    While we hit 150,000 in #COVID19 deaths, the P...   \n",
       "239999    This too shall pass #Covid_19 . May remain sta...   \n",
       "\n",
       "         reply_to_screen_name is_quote is_retweet  \\\n",
       "index_no                                            \n",
       "0                         NaN    False       True   \n",
       "1                         NaN    False       True   \n",
       "2                         NaN    False       True   \n",
       "3                         NaN    False       True   \n",
       "4                         NaN    False       True   \n",
       "...                       ...      ...        ...   \n",
       "239995                    NaN     TRUE       TRUE   \n",
       "239996                    NaN    FALSE       TRUE   \n",
       "239997                    NaN     TRUE       TRUE   \n",
       "239998                    NaN    FALSE       TRUE   \n",
       "239999                    NaN    FALSE       TRUE   \n",
       "\n",
       "                                                   hashtags      country  \\\n",
       "index_no                                                                   \n",
       "0                              WuhanCoronaVirus KillerCuomo           us   \n",
       "1                                                     Trump           us   \n",
       "2                                                   COVID19           us   \n",
       "3                       Pakistan GreenStimulus Nature Green           us   \n",
       "4                            coronavirus COVID__19 COVIDãƒ¼19           us   \n",
       "...                                                     ...          ...   \n",
       "239995                                               MastÐµr  new_zealand   \n",
       "239996    TheFive Trump2020 KAG2020 mondaythoughts COVID...  new_zealand   \n",
       "239997               COVID19 Christians persecution Nigeria  new_zealand   \n",
       "239998                                              COVID19  new_zealand   \n",
       "239999                                   Covid_19 HopeAlive  new_zealand   \n",
       "\n",
       "                                                 token_data  \\\n",
       "index_no                                                      \n",
       "0         ['Remember', 'the', '#', 'WuhanCoronaVirus', '...   \n",
       "1         ['My', 'sources', '@', 'WhiteHouse', 'say', '2...   \n",
       "2         ['I', \"'ll\", 'venture', 'a', 'wild', 'guess', ...   \n",
       "3         ['#', 'Pakistan', '(', '#', 'GreenStimulus', '...   \n",
       "4         ['ðŸ‡ºðŸ‡¸', 'PandÃ©mie', 'de', '#', 'coronavirus', '...   \n",
       "...                                                     ...   \n",
       "239995    ['Aa', 'Likes', ',', 'Retweets', 'yentra', 'ðŸ™'...   \n",
       "239996    ['Very', 'interesting', 'Any', 'thoughts', '?'...   \n",
       "239997    ['As', 'we', 'deal', 'with', '#', 'COVID19', '...   \n",
       "239998    ['While', 'we', 'hit', '150,000', 'in', '#', '...   \n",
       "239999    ['This', 'too', 'shall', 'pass', '#', 'Covid_1...   \n",
       "\n",
       "                                            lemmatized_data  \\\n",
       "index_no                                                      \n",
       "0         ['Remember', 'the', '#', 'WuhanCoronaVirus', '...   \n",
       "1         ['My', 'source', '@', 'WhiteHouse', 'say', '2'...   \n",
       "2         ['I', \"'ll\", 'venture', 'a', 'wild', 'guess', ...   \n",
       "3         ['#', 'Pakistan', '(', '#', 'GreenStimulus', '...   \n",
       "4         ['ðŸ‡ºðŸ‡¸', 'PandÃ©mie', 'de', '#', 'coronavirus', '...   \n",
       "...                                                     ...   \n",
       "239995    ['Aa', 'Likes', ',', 'Retweets', 'yentra', 'ðŸ™'...   \n",
       "239996    ['Very', 'interest', 'Any', 'thought', '?', '#...   \n",
       "239997    ['As', 'we', 'deal', 'with', '#', 'COVID19', '...   \n",
       "239998    ['While', 'we', 'hit', '150,000', 'in', '#', '...   \n",
       "239999    ['This', 'too', 'shall', 'pas', '#', 'Covid_19...   \n",
       "\n",
       "                                          filtered_sentence  \\\n",
       "index_no                                                      \n",
       "0         ['Remember', '#', 'WuhanCoronaVirus', '?', 'Th...   \n",
       "1         ['My', 'source', '@', 'WhiteHouse', 'say', '2'...   \n",
       "2         ['I', \"'ll\", 'venture', 'wild', 'guess', ':', ...   \n",
       "3         ['#', 'Pakistan', '(', '#', 'GreenStimulus', '...   \n",
       "4         ['ðŸ‡ºðŸ‡¸', 'PandÃ©mie', 'de', '#', 'coronavirus', '...   \n",
       "...                                                     ...   \n",
       "239995    ['Aa', 'Likes', ',', 'Retweets', 'yentra', 'ðŸ™'...   \n",
       "239996    ['Very', 'interest', 'Any', 'thought', '?', '#...   \n",
       "239997    ['As', 'deal', '#', 'COVID19', \"n't\", 'forget'...   \n",
       "239998    ['While', 'hit', '150,000', '#', 'COVID19', 'd...   \n",
       "239999    ['This', 'shall', 'pas', '#', 'Covid_19', '.',...   \n",
       "\n",
       "                                                 text_clean  \n",
       "index_no                                                     \n",
       "0         Remember  WuhanCoronaVirus  The pandemic great...  \n",
       "1          source  WhiteHouse say  tactic use get Americ...  \n",
       "2           venture wild guess   run USA  COVID crisis  ...  \n",
       "3          Pakistan   GreenStimulus   Nature protection ...  \n",
       "4          Pandmie   coronavirus   pasteur amricains qui...  \n",
       "...                                                     ...  \n",
       "239995                     Likes  Retweets yentra    MastÐµr  \n",
       "239996    Very interest Any thought   TheFive  Trump  KA...  \n",
       "239997     deal  COVID  forget  Christians   persecution...  \n",
       "239998    While hit   COVID death  President golf First ...  \n",
       "239999    This shall pas  Covid  May remain stand midst ...  \n",
       "\n",
       "[240000 rows x 10 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv(\"cleaned_final_textCleaned_data.csv\",low_memory=False)\n",
    "df=df.rename(columns={\"Unnamed: 0\": \"index_no\"})\n",
    "df = df.set_index(\"index_no\")\n",
    "df[\"text_clean\"]=df[\"text_clean\"].values.astype('U')\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9dbd52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061e149f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c021c9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(df[\"text_clean\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "25095029",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_tokenized_text(train,column):\n",
    "    \n",
    "    total_words = len(tokenizer.word_index) + 1\n",
    "    tokenized_text = []\n",
    "    for line in tqdm(train[column]):\n",
    "        tokenized_text.append(tokenizer.texts_to_sequences([line])[0])\n",
    "    tokenized_test = []\n",
    "    \n",
    "    return tokenized_text,total_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8469e669",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 240000/240000 [00:03<00:00, 78441.94it/s]\n"
     ]
    }
   ],
   "source": [
    "train_text,total_words = return_tokenized_text(df,\"text_clean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d069589a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "394323"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6c4051ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = max([len(x) for x in train_text])\n",
    "maxlen=maxlen-1\n",
    "padded_sequences_tr = pad_sequences(train_text,maxlen=maxlen,padding='post')\n",
    "padded_sequences_te = pad_sequences(train_text,maxlen=maxlen,padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "694e1fbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maxlen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5f9ec61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "df[\"country\"] = le.fit_transform(df['country'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4f80b0b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['australia', 'canada', 'ireland', 'new_zealand', 'uk', 'us']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(le.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "15543496",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " le.transform(['australia', 'canada', 'ireland', 'new_zealand', 'uk', 'us'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b2783cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_train = to_categorical(df[\"country\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e49984",
   "metadata": {},
   "source": [
    "#### Reference:\n",
    "https://www.tensorflow.org/text/tutorials/text_classification_rnn\n",
    "\n",
    "https://www.analyticsvidhya.com/blog/2021/06/lstm-for-text-classification/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "83fc0a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(total_words, max_sequence_len):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(total_words, 100, input_length=max_sequence_len))\n",
    "    model.add(Bidirectional(LSTM(150)))\n",
    "    model.add(Dense(6,activation='softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a478f728",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maxlen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5310264b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lstm = create_model(total_words, maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3dae81e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "7500/7500 [==============================] - 168s 22ms/step - loss: 1.4600 - accuracy: 0.4126\n",
      "Epoch 2/2\n",
      "7500/7500 [==============================] - 162s 22ms/step - loss: 0.9042 - accuracy: 0.6753\n"
     ]
    }
   ],
   "source": [
    "history = model_lstm.fit(padded_sequences_tr, labels_train, epochs=2, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaedc38d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4dacb01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3890cb3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3e218988",
   "metadata": {},
   "source": [
    "### Reading and Cleaning the Test Data set(for Kaggle submission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "34e4d5f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>reply_to_screen_name</th>\n",
       "      <th>is_quote</th>\n",
       "      <th>is_retweet</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>Id</th>\n",
       "      <th>token_data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ethical investing is not optional anymore, say...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>covid19</td>\n",
       "      <td>0</td>\n",
       "      <td>[Ethical, investing, is, not, optional, anymor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>#COVID19 | Suite Ã  la confÃ©rence de presse du ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>COVID19</td>\n",
       "      <td>1</td>\n",
       "      <td>[#, COVID19, |, Suite, Ã , la, confÃ©rence, de, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Yesterday, I had a live discussion with @Steve...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>COVID19</td>\n",
       "      <td>2</td>\n",
       "      <td>[Yesterday, ,, I, had, a, live, discussion, wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Nepal - #Coronavirus cases up 24% in a week. D...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>Coronavirus</td>\n",
       "      <td>3</td>\n",
       "      <td>[Nepal, -, #, Coronavirus, cases, up, 24, %, i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>American economy jumped up a %  big news story...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>LysolAndCloroxSales</td>\n",
       "      <td>4</td>\n",
       "      <td>[American, economy, jumped, up, a, %, big, new...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text reply_to_screen_name  \\\n",
       "0  Ethical investing is not optional anymore, say...                  NaN   \n",
       "1  #COVID19 | Suite Ã  la confÃ©rence de presse du ...                  NaN   \n",
       "2  Yesterday, I had a live discussion with @Steve...                  NaN   \n",
       "3  Nepal - #Coronavirus cases up 24% in a week. D...                  NaN   \n",
       "4  American economy jumped up a %  big news story...                  NaN   \n",
       "\n",
       "   is_quote  is_retweet             hashtags  Id  \\\n",
       "0     False       False              covid19   0   \n",
       "1     False        True              COVID19   1   \n",
       "2     False        True              COVID19   2   \n",
       "3     False        True          Coronavirus   3   \n",
       "4     False       False  LysolAndCloroxSales   4   \n",
       "\n",
       "                                          token_data  \n",
       "0  [Ethical, investing, is, not, optional, anymor...  \n",
       "1  [#, COVID19, |, Suite, Ã , la, confÃ©rence, de, ...  \n",
       "2  [Yesterday, ,, I, had, a, live, discussion, wi...  \n",
       "3  [Nepal, -, #, Coronavirus, cases, up, 24, %, i...  \n",
       "4  [American, economy, jumped, up, a, %, big, new...  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dft=pd.read_csv(\"test_data.csv\")\n",
    "\n",
    "str_data=dft['text'].astype(str).values.tolist()\n",
    "token_data = [nltk.word_tokenize(i) for i in str_data]\n",
    "\n",
    "dft['token_data']=token_data\n",
    "dft.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f65e64dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 60000/60000 [08:03<00:00, 124.10it/s]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus.reader.wordnet import VERB, NOUN, ADJ, ADV\n",
    "lemmatized_data=[]\n",
    "def mapping_pos(w):\n",
    "    t = nltk.pos_tag([w])[0][1][0].upper()\n",
    "    dict_pos_map = {\n",
    "        # Look for N in the POS tag because all nouns begin with N\n",
    "        'N': NOUN,\n",
    "        # Look for V in the POS tag because all nouns begin with V\n",
    "        'V':VERB,\n",
    "        # Look for J in the POS tag because all nouns begin with J\n",
    "        'J' : ADJ,\n",
    "        # Look for R in the POS tag because all nouns begin with R\n",
    "        'R':ADV  \n",
    "    }\n",
    "    return dict_pos_map.get(t, NOUN)\n",
    "\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lmtzr = WordNetLemmatizer()\n",
    "for i in tqdm(range(0, len(str_data)),desc=\"Progress\"):\n",
    "    x=[lmtzr.lemmatize(w, mapping_pos(w)) for w in nltk.word_tokenize(str_data[i])]\n",
    "    lemmatized_data.append(x)\n",
    "    \n",
    "dft['lemmatized_data']=lemmatized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "69e97db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dft.to_csv('Test_Data_lemmatized_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "81abc695",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 60000/60000 [00:00<00:00, 322490.34it/s]\n"
     ]
    }
   ],
   "source": [
    "filtered_sentence = []\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "for i in tqdm(range(0,len(lemmatized_data)),desc=\"Progress\"):\n",
    "    sentence = lemmatized_data[i]\n",
    "    filtered_sentence.append([w for w in sentence if not w in stop_words])\n",
    "\n",
    "dft['filtered_sentence']= filtered_sentence    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0d81e04e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 60000/60000 [00:02<00:00, 27281.66it/s]\n"
     ]
    }
   ],
   "source": [
    "clean_t = (dft['filtered_sentence']).astype(str).to_list()\n",
    "text_clean=[]\n",
    "\n",
    "emoji = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                               u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U000024C2-\\U0001F251\"\n",
    "                               u\"\\U0001f926-\\U0001f937\"\n",
    "                               u\"\\U00010000-\\U0010ffff\"\n",
    "                               u\"\\u2640-\\u2642\"\n",
    "                               u\"\\u2600-\\u2B55\"\n",
    "                               u\"\\u200d\"\n",
    "                               u\"\\u23cf\"\n",
    "                               u\"\\u23e9\"\n",
    "                               u\"\\u231a\"\n",
    "                               u\"\\ufe0f\"  # dingbats\n",
    "                               u\"\\u3030\"\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "\n",
    "puncts = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', 'â€¢',  '~', '@', 'Â£',\n",
    " 'Â·', '_', '{', '}', 'Â©', '^', 'Â®', '`',  '<', 'â†’', 'Â°', 'â‚¬', 'â„¢', 'â€º',  'â™¥', 'â†', 'Ã—', 'Â§', 'â€³', 'â€²', 'Ã‚', 'â–ˆ', 'Â½', 'Ã ', 'â€¦',\n",
    " 'â€œ', 'â˜…', 'â€', 'â€“', 'â—', 'Ã¢', 'â–º', 'âˆ’', 'Â¢', 'Â²', 'Â¬', 'â–‘', 'Â¶', 'â†‘', 'Â±', 'Â¿', 'â–¾', 'â•', 'Â¦', 'â•‘', 'â€•', 'Â¥', 'â–“', 'â€”', 'â€¹', 'â”€',\n",
    " 'â–’', 'ï¼š', 'Â¼', 'âŠ•', 'â–¼', 'â–ª', 'â€ ', 'â– ', 'â€™', 'â–€', 'Â¨', 'â–„', 'â™«', 'â˜†', 'Ã©', 'Â¯', 'â™¦', 'Â¤', 'â–²', 'Ã¨', 'Â¸', 'Â¾', 'Ãƒ', 'â‹…', 'â€˜', 'âˆž',\n",
    " 'âˆ™', 'ï¼‰', 'â†“', 'ã€', 'â”‚', 'ï¼ˆ', 'Â»', 'ï¼Œ', 'â™ª', 'â•©', 'â•š', 'Â³', 'ãƒ»', 'â•¦', 'â•£', 'â•”', 'â•—', 'â–¬', 'â¤', 'Ã¯', 'Ã˜', 'Â¹', 'â‰¤', 'â€¡', 'âˆš', ]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for i in tqdm(range(0,len(filtered_sentence)),desc=\"Progress\"):\n",
    "    #removing words with characters less than 2\n",
    "    clean=re.sub(r'\\b\\w{1,2}\\b',\"\",clean_t[i])\n",
    "    #removing punctuations\n",
    "    clean=re.sub(r'[^\\w\\s]', '',clean)\n",
    "    #removing numbers\n",
    "    clean=re.sub(r'\\d+', '', clean)\n",
    "    #removing links\n",
    "    clean=re.sub(r'http\\S+', '', clean)   \n",
    "    #removing emojis\n",
    "    clean= emoji.sub(r'', clean)\n",
    "    #joining the list of words back inot a joined string joined by space\n",
    "    for punct in puncts:\n",
    "            if punct in clean:\n",
    "                    clean = clean.replace(punct,'')\n",
    "    \n",
    "    text_clean.append(clean)\n",
    "    \n",
    "dft['text_clean']=text_clean\n",
    "dft.head()\n",
    "\n",
    "dft.to_csv('Test_Data_final_Cleaned_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f8b9c8",
   "metadata": {},
   "source": [
    "### Prediciting the Country on the Test Data set using the the Model LSTM trained above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "342063e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>reply_to_screen_name</th>\n",
       "      <th>is_quote</th>\n",
       "      <th>is_retweet</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>Id</th>\n",
       "      <th>token_data</th>\n",
       "      <th>lemmatized_data</th>\n",
       "      <th>filtered_sentence</th>\n",
       "      <th>text_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ethical investing is not optional anymore, say...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>covid19</td>\n",
       "      <td>0</td>\n",
       "      <td>['Ethical', 'investing', 'is', 'not', 'optiona...</td>\n",
       "      <td>['Ethical', 'invest', 'be', 'not', 'optional',...</td>\n",
       "      <td>['Ethical', 'invest', 'optional', 'anymore', '...</td>\n",
       "      <td>Ethical invest optional anymore  say SRI  Juli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>#COVID19 | Suite Ã  la confÃ©rence de presse du ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>COVID19</td>\n",
       "      <td>1</td>\n",
       "      <td>['#', 'COVID19', '|', 'Suite', 'Ã ', 'la', 'con...</td>\n",
       "      <td>['#', 'COVID19', '|', 'Suite', 'Ã ', 'la', 'con...</td>\n",
       "      <td>['#', 'COVID19', '|', 'Suite', 'Ã ', 'la', 'con...</td>\n",
       "      <td>COVID  Suite   confrence  presse  Premier min...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Yesterday, I had a live discussion with @Steve...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>COVID19</td>\n",
       "      <td>2</td>\n",
       "      <td>['Yesterday', ',', 'I', 'had', 'a', 'live', 'd...</td>\n",
       "      <td>['Yesterday', ',', 'I', 'have', 'a', 'live', '...</td>\n",
       "      <td>['Yesterday', ',', 'I', 'live', 'discussion', ...</td>\n",
       "      <td>Yesterday   live discussion  SteveFDA  COVID  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Nepal - #Coronavirus cases up 24% in a week. D...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>Coronavirus</td>\n",
       "      <td>3</td>\n",
       "      <td>['Nepal', '-', '#', 'Coronavirus', 'cases', 'u...</td>\n",
       "      <td>['Nepal', '-', '#', 'Coronavirus', 'case', 'up...</td>\n",
       "      <td>['Nepal', '-', '#', 'Coronavirus', 'case', '24...</td>\n",
       "      <td>Nepal   Coronavirus case   week  Deaths    Tot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>American economy jumped up a %  big news story...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>LysolAndCloroxSales</td>\n",
       "      <td>4</td>\n",
       "      <td>['American', 'economy', 'jumped', 'up', 'a', '...</td>\n",
       "      <td>['American', 'economy', 'jumped', 'up', 'a', '...</td>\n",
       "      <td>['American', 'economy', 'jumped', '%', 'big', ...</td>\n",
       "      <td>American economy jumped  big news story  whole...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59995</th>\n",
       "      <td>Who knew all we had to do was eat a tennerâ€™s w...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>Covid19</td>\n",
       "      <td>59995</td>\n",
       "      <td>['Who', 'knew', 'all', 'we', 'had', 'to', 'do'...</td>\n",
       "      <td>['Who', 'knew', 'all', 'we', 'have', 'to', 'do...</td>\n",
       "      <td>['Who', 'knew', 'eat', 'tenner', 'â€™', 'worth',...</td>\n",
       "      <td>Who knew eat tenner  worth grub avoid contract...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59996</th>\n",
       "      <td>Our death toll of more than 160,000 souls tell...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>coronavirus</td>\n",
       "      <td>59996</td>\n",
       "      <td>['Our', 'death', 'toll', 'of', 'more', 'than',...</td>\n",
       "      <td>['Our', 'death', 'toll', 'of', 'more', 'than',...</td>\n",
       "      <td>['Our', 'death', 'toll', '160,000', 'soul', 't...</td>\n",
       "      <td>Our death toll  soul tell   coronavirus never ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59997</th>\n",
       "      <td>Apparently, Donald Trump doesn't care about Am...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>COVID19</td>\n",
       "      <td>59997</td>\n",
       "      <td>['Apparently', ',', 'Donald', 'Trump', 'does',...</td>\n",
       "      <td>['Apparently', ',', 'Donald', 'Trump', 'do', \"...</td>\n",
       "      <td>['Apparently', ',', 'Donald', 'Trump', \"n't\", ...</td>\n",
       "      <td>Apparently  Donald Trump  care Americans live ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59998</th>\n",
       "      <td>Is #HulaHooping the perfect #SocialDistancing ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>HulaHooping SocialDistancing London</td>\n",
       "      <td>59998</td>\n",
       "      <td>['Is', '#', 'HulaHooping', 'the', 'perfect', '...</td>\n",
       "      <td>['Is', '#', 'HulaHooping', 'the', 'perfect', '...</td>\n",
       "      <td>['Is', '#', 'HulaHooping', 'perfect', '#', 'So...</td>\n",
       "      <td>HulaHooping perfect  SocialDistancing activi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59999</th>\n",
       "      <td>Leaving the #WHO is a monumentally stupid deci...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>WHO</td>\n",
       "      <td>59999</td>\n",
       "      <td>['Leaving', 'the', '#', 'WHO', 'is', 'a', 'mon...</td>\n",
       "      <td>['Leaving', 'the', '#', 'WHO', 'be', 'a', 'mon...</td>\n",
       "      <td>['Leaving', '#', 'WHO', 'monumentally', 'stupi...</td>\n",
       "      <td>Leaving  WHO monumentally stupid decision deat...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>60000 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text reply_to_screen_name  \\\n",
       "0      Ethical investing is not optional anymore, say...                  NaN   \n",
       "1      #COVID19 | Suite Ã  la confÃ©rence de presse du ...                  NaN   \n",
       "2      Yesterday, I had a live discussion with @Steve...                  NaN   \n",
       "3      Nepal - #Coronavirus cases up 24% in a week. D...                  NaN   \n",
       "4      American economy jumped up a %  big news story...                  NaN   \n",
       "...                                                  ...                  ...   \n",
       "59995  Who knew all we had to do was eat a tennerâ€™s w...                  NaN   \n",
       "59996  Our death toll of more than 160,000 souls tell...                  NaN   \n",
       "59997  Apparently, Donald Trump doesn't care about Am...                  NaN   \n",
       "59998  Is #HulaHooping the perfect #SocialDistancing ...                  NaN   \n",
       "59999  Leaving the #WHO is a monumentally stupid deci...                  NaN   \n",
       "\n",
       "       is_quote  is_retweet                             hashtags     Id  \\\n",
       "0         False       False                              covid19      0   \n",
       "1         False        True                              COVID19      1   \n",
       "2         False        True                              COVID19      2   \n",
       "3         False        True                          Coronavirus      3   \n",
       "4         False       False                  LysolAndCloroxSales      4   \n",
       "...         ...         ...                                  ...    ...   \n",
       "59995     False        True                              Covid19  59995   \n",
       "59996     False        True                          coronavirus  59996   \n",
       "59997     False        True                              COVID19  59997   \n",
       "59998     False       False  HulaHooping SocialDistancing London  59998   \n",
       "59999      True        True                                  WHO  59999   \n",
       "\n",
       "                                              token_data  \\\n",
       "0      ['Ethical', 'investing', 'is', 'not', 'optiona...   \n",
       "1      ['#', 'COVID19', '|', 'Suite', 'Ã ', 'la', 'con...   \n",
       "2      ['Yesterday', ',', 'I', 'had', 'a', 'live', 'd...   \n",
       "3      ['Nepal', '-', '#', 'Coronavirus', 'cases', 'u...   \n",
       "4      ['American', 'economy', 'jumped', 'up', 'a', '...   \n",
       "...                                                  ...   \n",
       "59995  ['Who', 'knew', 'all', 'we', 'had', 'to', 'do'...   \n",
       "59996  ['Our', 'death', 'toll', 'of', 'more', 'than',...   \n",
       "59997  ['Apparently', ',', 'Donald', 'Trump', 'does',...   \n",
       "59998  ['Is', '#', 'HulaHooping', 'the', 'perfect', '...   \n",
       "59999  ['Leaving', 'the', '#', 'WHO', 'is', 'a', 'mon...   \n",
       "\n",
       "                                         lemmatized_data  \\\n",
       "0      ['Ethical', 'invest', 'be', 'not', 'optional',...   \n",
       "1      ['#', 'COVID19', '|', 'Suite', 'Ã ', 'la', 'con...   \n",
       "2      ['Yesterday', ',', 'I', 'have', 'a', 'live', '...   \n",
       "3      ['Nepal', '-', '#', 'Coronavirus', 'case', 'up...   \n",
       "4      ['American', 'economy', 'jumped', 'up', 'a', '...   \n",
       "...                                                  ...   \n",
       "59995  ['Who', 'knew', 'all', 'we', 'have', 'to', 'do...   \n",
       "59996  ['Our', 'death', 'toll', 'of', 'more', 'than',...   \n",
       "59997  ['Apparently', ',', 'Donald', 'Trump', 'do', \"...   \n",
       "59998  ['Is', '#', 'HulaHooping', 'the', 'perfect', '...   \n",
       "59999  ['Leaving', 'the', '#', 'WHO', 'be', 'a', 'mon...   \n",
       "\n",
       "                                       filtered_sentence  \\\n",
       "0      ['Ethical', 'invest', 'optional', 'anymore', '...   \n",
       "1      ['#', 'COVID19', '|', 'Suite', 'Ã ', 'la', 'con...   \n",
       "2      ['Yesterday', ',', 'I', 'live', 'discussion', ...   \n",
       "3      ['Nepal', '-', '#', 'Coronavirus', 'case', '24...   \n",
       "4      ['American', 'economy', 'jumped', '%', 'big', ...   \n",
       "...                                                  ...   \n",
       "59995  ['Who', 'knew', 'eat', 'tenner', 'â€™', 'worth',...   \n",
       "59996  ['Our', 'death', 'toll', '160,000', 'soul', 't...   \n",
       "59997  ['Apparently', ',', 'Donald', 'Trump', \"n't\", ...   \n",
       "59998  ['Is', '#', 'HulaHooping', 'perfect', '#', 'So...   \n",
       "59999  ['Leaving', '#', 'WHO', 'monumentally', 'stupi...   \n",
       "\n",
       "                                              text_clean  \n",
       "0      Ethical invest optional anymore  say SRI  Juli...  \n",
       "1       COVID  Suite   confrence  presse  Premier min...  \n",
       "2      Yesterday   live discussion  SteveFDA  COVID  ...  \n",
       "3      Nepal   Coronavirus case   week  Deaths    Tot...  \n",
       "4      American economy jumped  big news story  whole...  \n",
       "...                                                  ...  \n",
       "59995  Who knew eat tenner  worth grub avoid contract...  \n",
       "59996  Our death toll  soul tell   coronavirus never ...  \n",
       "59997  Apparently  Donald Trump  care Americans live ...  \n",
       "59998    HulaHooping perfect  SocialDistancing activi...  \n",
       "59999  Leaving  WHO monumentally stupid decision deat...  \n",
       "\n",
       "[60000 rows x 10 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dft=pd.read_csv(\"Test_Data_final_Cleaned_data.csv\",low_memory=False)\n",
    "dft=dft.drop(columns=[\"Unnamed: 0\"])\n",
    "dft[\"text_clean\"]=dft[\"text_clean\"].values.astype('U')\n",
    "dft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "da62c84c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 60000/60000 [00:00<00:00, 73174.48it/s]\n"
     ]
    }
   ],
   "source": [
    "test_text,total_words = return_tokenized_text(dft,\"text_clean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "66bdde3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_sequences_te = pad_sequences(test_text,maxlen=maxlen,padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c87f121d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model_lstm.predict(padded_sequences_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "96d1b3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = np.argmax(pred, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9fad6540",
   "metadata": {},
   "outputs": [],
   "source": [
    "final = pd.DataFrame()\n",
    "final['id'] = dft['Id']\n",
    "final_prediction = le.inverse_transform(pred)\n",
    "final['category'] = final_prediction\n",
    "final.to_csv('Test_Data_predictions_data.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb324ac1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06cc9e87",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
